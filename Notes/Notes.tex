\documentclass[titlepage, fleqn, a4paper, 12pt, twoside]{article}
\usepackage{geometry}
\usepackage{exsheets} %question and solution environments
\usepackage{amsmath, amssymb, amsthm} %standard AMS packages
\usepackage[utf8]{inputenc}
\usepackage{esint} %integral signs
\usepackage{marginnote} %marginnotes
\usepackage{gensymb} %miscellaneous symbols
\usepackage{commath} %differential symbols
\usepackage{xcolor} %colours
\usepackage{cancel} %cancelling terms
\usepackage[free-standing-units,space-before-unit]{siunitx} %formatting units
	\sisetup
	{
		per-mode=fraction,
		fraction-function=\frac
	}
\usepackage{tikz, pgfplots} %diagrams
	\usetikzlibrary{calc, hobby, patterns, intersections, angles, quotes, spy}
	\usetikzlibrary{circuits.logic.US,circuits.logic.IEC}
\usepackage{graphicx} %inserting graphics
\usepackage{imakeidx}
\makeindex
\usepackage{hyperref} %hyperlinks
\usepackage{datetime} %date and time
\usepackage{enumerate, enumitem} %numbered lists
\usepackage{float} %inserting floats
\usepackage[american voltages]{circuitikz} %circuit diagrams
\usepackage{setspace} %double spacing
\usepackage{microtype} %micro-typography
\usepackage{listings} %formatting code
	\lstset{language=Matlab}
	\lstdefinestyle{standardMatlab}
	{
		belowcaptionskip=1\baselineskip,
		breaklines=true,
		frame=L,
		xleftmargin=\parindent,
		language=C,
		showstringspaces=false,
		basicstyle=\footnotesize\ttfamily,
		keywordstyle=\bfseries\color{green!40!black},
		commentstyle=\itshape\color{purple!40!black},
		identifierstyle=\color{blue},
		stringstyle=\color{orange},
	}
\usepackage{booktabs}
\usepackage{multirow}
\usepackage{todonotes}
\usepackage[noabbrev,capitalize]{cleveref}
\usepackage[section]{placeins}
\usepackage[style=numeric, backend=biber]{biblatex}
\usepackage{adjustbox}
\usepackage{algpseudocode} %algorithms
\usepackage{algorithm} %algorithms

% \bibliography{<mybibfile>}% ONLY selects .bib file; syntax for version <= 1.1b
\addbibresource{bibliography.bib}% Syntax for version >= 1.2

\newcommand\numberthis{\addtocounter{equation}{1}\tag{\theequation}} %adds numbers to specific equations in non-numbered list of equations

\DeclareMathAlphabet{\mathcal}{OT1}{pzc}{m}{it}

\theoremstyle{definition}
\newtheorem{example}{Example}
\newtheorem{definition}{Definition}

\theoremstyle{theorem}
\newtheorem{theorem}{Theorem}
\newtheorem{law}{Law}

\tikzset{
block/.style = {draw, rectangle, minimum height=3em, minimum width=3em},
tmp/.style  = {coordinate},
sum/.style = {draw, circle, node distance=1cm},
input/.style = {coordinate},
output/.style = {coordinate},
pinstyle/.style = {pin edge={to-,thin,black}}
}

\makeatletter
\@addtoreset{section}{part} %resets section numbers in new part
\makeatother

\newcommand\blfootnote[1]{%
	\begingroup
	\renewcommand\thefootnote{}\footnote{#1}%
	\addtocounter{footnote}{-1}%
	\endgroup
}

\renewcommand{\marginfont}{\scriptsize \color{blue}}

\renewcommand{\tilde}{\widetilde}

\def\doubleunderline#1{\underline{\underline{#1}}}

\SetupExSheets{solution/print = true} %prints all solutions by default

\DeclareMathOperator{\FT}{\mathcal{F}}
\DeclareMathOperator{\IFT}{\mathcal{F}^{-1}}

\DeclareMathOperator{\Q}{\mathcal{Q}}

\newcommand{\NOT}{\overline}

\DeclareMathOperator{\rect}{\mathrm{rect}}
\DeclareMathOperator{\sinc}{\mathrm{sinc}}

\DeclareMathOperator{\vspan}{\mathrm{span}}

\DeclareMathOperator{\argmin}{\mathrm{argmin}}

%opening
\title{Digital Processing of Single and Multidimensional Signals}
\author{Aakash Jog}
\date{2017-18}

\begin{document}

\pagenumbering{roman}
\begin{titlepage}
\newgeometry{margin=0cm}
\maketitle
\end{titlepage}
\restoregeometry
%\setlength{\mathindent}{0pt}

\blfootnote
{
	\begin{figure}[H]
		\includegraphics[height = 12pt]{cc.pdf}
		\includegraphics[height = 12pt]{by.pdf}
		\includegraphics[height = 12pt]{nc.pdf}
		\includegraphics[height = 12pt]{sa.pdf}
	\end{figure}
	This work is licensed under the Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International License. To view a copy of this license, visit \url{http://creativecommons.org/licenses/by-nc-sa/4.0/}.
} %CC-BY-NC-SA license

\tableofcontents

\clearpage
\section{Lecturer Information}

\textbf{Dr. Raja Giryes}\\
~\\
E-mail: \href{mailto:raja@tauex.tau.ac.il}{raja@tauex.tau.ac.il}\\
~\\

\section{Grading}

\begin{enumerate}
	\item Homework (MATLAB or NumPy): 20\%
	\item Final exam: 20\%
\end{enumerate}

\section{General Structure}

\begin{enumerate}
	\item Multidimensional Fourier analysis
	\item Hilbert spaces and generalized sampling
	\item Spline interpolation
	\item Filter banks and wavelets
	\item Sparsity
	\item Low rank theory or collaborative filtering
	\item Phase retrieval
\end{enumerate}

\clearpage
\pagenumbering{arabic}

\part{Basic Definition and Theorems}

\begin{definition}[Fourier transform]
	For an LTI system, the Fourier transform is defined as
	\begin{align*}
		\FT\left\{ x(t) \right\}j &= \left\langle x,e^{2 \pi j f t} \right\rangle\\
		&= \int\limits_{t \in R^d} x(t) e^{-2 j f^T t} \dif t
	\end{align*}
	\label{def:Fourier_transform}
	\index{transform!Fourier}
\end{definition}

\begin{definition}[Tensor product]
	\begin{align*}
		(x_1 \otimes \dots \otimes x_d)(t) &= x_1(t_1) \cdot \dots \cdot x_d(t_d)
	\end{align*}
	for $t \in R^d$.
	\index{product!tensor}
\end{definition}

\begin{definition}
	The Fourier transform of a tensor product is defined as
	\begin{align*}
		\FT\left\{ x_1 \otimes \dots \otimes x_d \right\}(f) &= \int\limits_{R^d} x_1(t_1) \cdot \dots \cdot x_d(t_d) e^{-2 \pi j (t_1 f_1 + \dots + t_d f_d)} \dif t_1 \dots \dif t_d\\
		&= \left( \FT\left\{ x_1(t) \right\}(f_1) \right) \dots \left( \FT\left\{ x_d(t) \right\}(f_d) \right)
	\end{align*}
	\index{transform!Fourier}
	\index{product!tensor}
\end{definition}

\begin{definition}[$\rect$]
	\begin{align*}
		\rect(t) &=
			\begin{cases}
				0 &;\quad |t| > \frac{1}{2}\\
				1 &;\quad |t| < \frac{1}{2}\\
			\end{cases}
	\end{align*}
	\index{standard functions!rect}
\end{definition}

\begin{definition}[$\sinc$]
	\begin{align*}
		\sinc(t) &= \frac{\sin(\pi t)}{\pi t}
	\end{align*}
	\index{standard functions!sinc}
\end{definition}

\begin{theorem}
	\begin{align*}
		\FT\left\{ \rect(t) \right\} &= \sinc(f)
	\end{align*}
	\index{standard functions!rect}
	\index{standard functions!sinc}
\end{theorem}

\begin{definition}
	A box function is defined to be the product of rect functions in multiple dimensions.
	\index{standard functions!box}
	\index{standard functions!rect}
\end{definition}

\begin{theorem}[Shifting in time]
	\begin{align*}
		\FT\left\{ D_p\left( x(t) \right) \right\}(f) &= e^{-2 \pi j f^T p} \FT\left\{ x(t) \right\}(f)
	\end{align*}
	where
	\begin{align*}
		D_p\left( x(t) \right) &= x(t - p)
	\end{align*}
	\label{thm:shifting_in_time}
	\index{shift!in time}
\end{theorem}

\begin{theorem}[Stretching in time]
	\begin{align*}
		\FT\left\{ S_A\left( x(t) \right) \right\}(f) &= \int\limits_{R^d} x(A t) e^{-j 2 \pi f^T t} \dif t\\
		&= \int\limits_{R^d} x(q) ^{-j 2 \pi f^T A^{-1} q} \frac{\dif q}{|\det A|}\\
		&= \frac{1}{|\det A|} \int\limits_{R^d}  x(q) e^{-j 2 \pi \left( A^{-T} f \right)^T q} \dif q\\
		&= \frac{\FT\left\{ x(t) \right\}\left( A^{-T} f \right)}{|\det A|}\\
		&= \frac{S^{A^{-T}} X(f)}{|\det A|}
	\end{align*}
	where
	\begin{align*}
		S_A\left( x(t) \right) &= x(A t)
	\end{align*}
	where $A$ is a matrix which may represent stretching, rotating, etc.\\
	Hence, if $A$ is equal to a scalar $a$,
	\begin{align*}
		\FT\left\{ x(a t) \right\} &= \frac{\FT\left\{ x(t) \right\}\left( \frac{f}{a} \right)}{|a|}
	\end{align*}
	\label{thm:stretching_in_time}
	\index{stretch!in time}
\end{theorem}

\begin{theorem}[Fourier transform of Gaussian]
	Using \cref{thm:stretching_in_time}, for a single dimensional Gaussian signal,
	\begin{align*}
		\FT\left\{ e^{-t^2} \right\} &= e^{-\pi f^2}
	\end{align*}
	and for a orthogonal multidimensional Gaussian,
	\begin{align*}
		\FT\left\{ e^{-t^T t} \right\} &= e^{-\pi f^T f}
	\end{align*}
	Hence, using \cref{thm:stretching_in_time}, for a general multidimensional Gaussian,
	\begin{align*}
		\FT\left\{ e^{-t C^{-1} t} \right\} &= \frac{1}{\sqrt{\det C}} e^{-\pi f^T C f}
	\end{align*}
	where $C$ is the covariance matrix.
	\label{thm:Fourier_transform_of_Gaussian}
	\index{transform!Fourier}
	\index{standard functions!Gaussian}
\end{theorem}

\begin{definition}[Unitary matrix]
	A matrix $A$ is said to be unitary if
	\begin{align*}
		A^{-1} &= A^*
	\end{align*}
	where $A^*$ is the conjugate transpose of $A$.
	\index{types of matrices!unitary}
	\index{conjugate transpose}
\end{definition}

\begin{theorem}[Rotation]
	Stretching with by a unitary matrix $A$ is equivalent to rotation, and hence denoting $S_A$ by $R_R$,
	\begin{align*}
		\FT\left\{ R_R\left( x(t) \right) \right\} &= R_R \FT\left\{ x(t) \right\}
	\end{align*}
	\label{thm:rotation}
	\index{stretch!in time}
	\index{rotation!in time}
\end{theorem}

\begin{definition}
	A dimension-reducing projection $P$ from $R^d$ to $R^{d - 1}$, is defined as
	\begin{align*}
		P\left( x(t) \right) &= \int\limits_{R^d} x(t_1,\dots,t_d) \dif t_d
	\end{align*}
	\index{projection!dimension-reducing}
\end{definition}

\begin{theorem}[Fourier transform of dimension reducing projection]
	The Fourier transform of a dimension reducing projection is
	\begin{align*}
		\FT\left\{ P\left( x(t) \right) \right\} &= \int\limits_{R^{d - 1}} \left( \int\limits_{R^d} x(t_1,\dots,t_d) \dif t_d \right) e^{-j 2 \pi (t_1 f_1 + \dots + t_{d - 1} f_{d - 1})} \dif t_1 \dots \dif t_{d - 1}\\
		&= \int\limits_{R^{d - 1}} \left( \int\limits_{R^d} x(t_1,\dots,t_d) e^{-j 2 \pi t_d (0)} \dif t_d \right) e^{-j 2 \pi (t_1 f_1 + \dots + t_{d - 1} f_{d - 1})} \dif t_1 \dots \dif t_{d - 1}\\
		&= \int\limits_{R^d} x(t) e^{-j 2 \pi f^T t} \dif t \Big|_{f_d = 0}\\
		&= \FT\left\{ x(t) \right\}(f_1,\dots,f_{d - 1},0)\\
		&= \Q\left\{ \FT\left\{ x(t) \right\} \right\}
	\end{align*}
	where $\Q$ is the slicing operator as in \cref{def:slicing_operator}.
	\label{thm:Fourier_transform_of_dimension_reducing_projection}
	\index{projection!dimension-reducing}
	\index{transform!Fourier}
\end{theorem}

\begin{definition}
	The slicing operator $\Q$ is defined as
	\begin{align*}
		\Q\left\{ X(f_1,\dots,f_d) \right\} &= X\left( f_1,\dots,f_{d - 1},0 \right)
	\end{align*}
	\label{def:slicing_operator}
	\index{standard operators!slicing}
\end{definition}

\clearpage
\part{Minimum Delay Systems and Group Delay}

\begin{theorem}
	Consider a real system $H(f)$ with input $x(t)$ and output $y(t)$.
	Assume $H(f)$ to be all-pass but with variable phase, i.e.
	\begin{align*}
		\left| H(f) \right| &= 1
	\end{align*}
	If $H(f)$ has a constant delay, i.e.
	\begin{align*}
		H(f) &= e^{-j 2 \pi f \alpha}
	\end{align*}
	then
	\begin{align*}
		y(t) &= x(t - \alpha)
	\end{align*}
\end{theorem}

\begin{definition}[Phase delay]
	The phase delay of a system $H(f)$ for a narrow band input is defined as
	\begin{align*}
		\tau_p &= -\frac{\angle H(f)}{2 \pi f_0}\\
		&= \frac{\varphi_0}{2 \pi f_0}
	\end{align*}
	where $\pm f_0$ are the central frequencies of the narrow band input.
	\label{def:phase_delay}
	\index{delay!phase}
\end{definition}

\begin{theorem}
	Consider a real system $H(f)$ with input $x(t)$ and output $y(t)$.
	Assume $H(f)$ to be all-pass but with variable phase, i.e.
	\begin{align*}
		\left| H(f) \right| &= 1
	\end{align*}
	Assume
	\begin{align*}
		x(t) &= s(t) \cos\left( 2 \pi f_0 t + \varphi \right)
	\end{align*}
	and equivalently,
	\begin{align*}
		X(f) &= \frac{1}{2} S(f - f_0) e^{j \varphi} + \frac{1}{2} S(f + f_0) e^{-j \varphi}
	\end{align*}
	If $H(f)$ has a phase delay, i.e.
	\begin{align*}
		\angle H(f) &=
			\begin{cases}
				-\varphi_0 &;\quad f > 0\\
				\varphi_0 &;\quad f < 0\\
			\end{cases}
	\end{align*}
	and equivalently,
	\begin{align*}
		H(f) &=
			\begin{cases}
				e^{-j \varphi_0} &;\quad f > 0\\
				e^{j \varphi_0} &;\quad f < 0\\
			\end{cases}
	\end{align*}
	Then,
	\begin{align*}
		Y(f) &= X(f) H(f)\\
		&= \frac{1}{2} S(f - f_0) e^{\varphi - \varphi_0} e^{j (\varphi - \varphi_0)} + \frac{1}{2} S(f + f_0) e^{-j (\varphi - \varphi_0)}
	\end{align*}
	and equivalently,
	\begin{align*}
		y(t) &= s(t) \cos(2 \pi f_0 t + \varphi - \varphi_0)\\
		&= s(t) \cos\left( 2 \pi f_0 (t - \tau_p) + \varphi \right)
	\end{align*}
	where $\tau_p$ is the phase delay as in \cref{def:phase_delay}.
\end{theorem}

\begin{definition}
	The group delay of a system $H(f)$ for a narrow band input is defined as
	\begin{align*}
		\tau_g(f) &= -\dod{}{f} \angle H(f)
	\end{align*}
	\label{def:group_delay}
	\index{delay!group}
\end{definition}

\begin{theorem}
	Consider a real system $H(f)$ with input $x(t)$ and output $y(t)$.
	Assume $H(f)$ to be all-pass but with variable phase, i.e.
	\begin{align*}
		\left| H(f) \right| &= 1
	\end{align*}
	Assume
	\begin{align*}
		x(t) &= s(t) \cos\left( 2 \pi f_0 t + \varphi \right)
	\end{align*}
	and equivalently,
	\begin{align*}
		X(f) &= \frac{1}{2} S(f - f_0) e^{j \varphi} + \frac{1}{2} S(f + f_0) e^{-j \varphi}
	\end{align*}
	If $H(f)$ has a non constant phase, such that
	\begin{align*}
		\angle H(\pm f_0) &= \mp \varphi_1
	\end{align*}
	then let $\mp \varphi_0$ be the points of intersection of the tangents to $\angle H(f)$ at $\pm f_0$.\\
	Hence, by Taylor's Formula,
	\begin{align*}
		\angle H(f) &\approx H(f_0) + (f - f_0) \left( \dod{}{f} \angle H(f) \Big|_{f = f_0} \right)\\
		&\approx \angle H(f_0) - (f - f_0) \tau_g(f_0)
	\end{align*}
	where $\tau_g$ is the group delay as in \cref{def:group_delay}.\\
	Similarly,
	\begin{align*}
		\angle H(f) &\approx \angle H(-f_0) - (f + f_0) \tau_g(-f_0)
	\end{align*}
	Hence,
	\begin{align*}
		-\varphi_1 &= \angle H(f_0)\\
		-\varphi_0 &= \angle H(f_0) + f_0 \tau_g(f_0)
	\end{align*}
	Hence, expressing the system as a sum of a system with a constant delay and a system with a constant phase, i.e.
	\begin{align*}
		H(f) &= H_A(f) + H_B(f)
	\end{align*}
	where
	\begin{align*}
		\angle H_A(f) &=
			\begin{cases}
				\varphi_0 &;\quad f < 0\\
				-\varphi_0 &;\quad f > 0\\
			\end{cases}\\
		\angle H_B(f) &= \tau_g(f_0)
	\end{align*}
	the output is
	\begin{align*}
		y(t) &= y_A(t) + y_B(t)\\
		&= s\left( t - \tau_g(f_0) \right) \cos\left( 2 \pi f_0 \left( t - \tau_g(f_0) \right) + \varphi - \varphi_0 \right)\\
		&= s\left( t - \tau_g(f_0) \right) \cos\left( 2 \pi f_0 (t - \tau_g - \tau_p) + \varphi \right)
	\end{align*}
	\index{delay!phase}
	\index{delay!group}
\end{theorem}

\clearpage
\part{Hilbert Spaces}

\section{Inner Product Spaces}

\begin{definition}[Inner product spaces]
	An inner product space is defined to be a space with an inner product such that
	\begin{align*}
		\langle x,y \rangle &= \langle y,x \rangle^*\\
		\langle x , a y + b z \rangle &= a \langle x,y \rangle + b \langle x,z \rangle\\
		\|x\| &= \langle x,x \rangle\\
		&\ge 0
	\end{align*}
	and $\langle x,x \rangle = 0$ iff $x = 0$.
	Also, $\|x - y\|$ is the distance between $x$ and $y$.
	\index{spaces!inner product}
\end{definition}

\begin{theorem}
	Let $M$ be an operator from the inner product space $H$ to the inner product space $S$.
	Then,
	\begin{align*}
		\langle M x , y \rangle &= \langle x , M^* y \rangle
	\end{align*}
	for all $x \in H$ and $y \in S$, and where
	\begin{align*}
		M^* &= \overline{M^T}
	\end{align*}
	is the complex transpose of $M$.
\end{theorem}

\begin{definition}[Hilbert space]
	An inner product, normed, complete vector space is said to be a Hilbert space.
	\index{spaces!Hilbert}
\end{definition}

\section{Operators and Transformations}

\begin{definition}[Linear operator]
	An operator $T$ is said to be bounded if and only if
	\begin{align*}
		T(a_1 x_1 + a_2 x_2) &= a_1 T(x_1) + a_2 T(x_2)
	\end{align*}
	\index{types of operators!linear}
\end{definition}

\begin{definition}[Adjoint of linear operation]
	The adjoint of a linear operation $T$ is defined to be $T^*$, the only bounded operation such that
	\begin{align*}
		\langle T x , y \rangle &= \langle x , T^* y \rangle
	\end{align*}
	for all $x \in H$, $y \in S$.\\
	\index{adjoint of linear operation}
\end{definition}

\begin{definition}[Unitary operator]
	An operator $T$ is said to be unitary if and only if
	\begin{align*}
		T T^* &= T^* T\\
		&= I
	\end{align*}
	where $T^*$ is the conjugate transpose of $T$.
	\index{types of operators!unitary}
\end{definition}

\begin{definition}[Hermitian operator]
	An operator $T$ is said to be Hermitian if and only if
	\begin{align*}
		T^* &= T
	\end{align*}
	where $T^*$ is the conjugate transpose of $T$.
	\index{types of operators!Hermitian}
\end{definition}

\begin{theorem}
	An operator $T$ from a Hilbert space $H$ to $H$ is Hermitian if and only if
	\begin{align*}
		\langle T x , x \rangle &\in \mathbb{R}
	\end{align*}
	for all $x \in H$.
	\index{types of operators!Hermitian}
\end{theorem}

\begin{definition}[Orthogonal complement of subspace]
	The orthogonal complement of a subspace $W$ is defined to be
	\begin{align*}
		W^{\perp} &= \left\{ x \Big| \langle x,y \rangle = 0 \quad \forall y \in W \right\}
	\end{align*}
	\index{orthogonal complement}
\end{definition}

\begin{definition}[Null space of linear operator]
	The null space of a linear operator $T$ is defined to be
	\begin{align*}
		N(T) &= \left\{ x \Big| T x = 0 \right\}
	\end{align*}
	\index{null space}
\end{definition}

\begin{definition}[Range of linear operator]
	The range of a linear operator $T$ is defined to be
	\begin{align*}
		R(T) &= \left\{ y \Big| T x = y \, , \, x \in H \right\}
	\end{align*}
	\index{range}
\end{definition}

\begin{theorem}
	For any linear operator $T$,
	\begin{align*}
		N\left( T^* \right) &= {R(T)}^{\perp}\\
		R\left( T^* \right) &= {N(T)}^{\perp}
	\end{align*}
	\index{null space}
	\index{range}
\end{theorem}

\begin{theorem}
	\begin{align*}
		N(T) &= N\left( T^* T \right)
	\end{align*}
	\index{null space}
\end{theorem}

\begin{theorem}
	For a linear operator $T: H \to S$,
	\begin{align*}
		H &= N(T) \cup N(T)^{\perp}
	\end{align*}
	where $N(T)$ is the null space of $T$.\\
	Additionally, if $S$ is a closed set,
	\begin{align*}
		S &= R(T)^{C} \cap R(T)^{\perp}
	\end{align*}
	where
	\begin{align*}
		R(T)^{C} &= \left( R(T)^{\perp} \right)^{\perp}
	\end{align*}
	is the closure of $R(T)$.
	\index{closure}
\end{theorem}

\begin{definition}[Injective transformation]
	A linear transformation/operation $T: H \to S$ is said to be injective (one-to-one) if
	\begin{align*}
		x \neq y &\iff T x \neq T(y)
	\end{align*}
	\index{injectivity}
	\index{one-to-one}
\end{definition}

\begin{definition}[Surjective transformation]
	A linear transformation/operation $T: H \to S$ is said to be surjective (onto) if
	\begin{align*}
		R(T) &= S
	\end{align*}
	\index{surjectivity}
	\index{onto}
\end{definition}

\begin{definition}[Bijective transformation]
	A linear transformation/operation $T: H \to S$ is said to be bijective if it is injective (one-to-one) and surjective (onto).
	\index{bijectivity}
	\index{one-to-one and onto}
\end{definition}

\section{Singular Value Decomposition (SVD)}

\begin{definition}[Eigenvalue decomposition]
	The eigenvalue decomposition for a symmetric (and hence also square) and Hermitian matrix $A$ is
	\begin{align*}
		A &= V \Lambda V^*
	\end{align*}
	where $\Lambda$ is a diagonal matrix with the eigenvalues of $A$ as diagonal elements, and $V$ contains the corresponding eigenvectors of $A$.
	\index{decomposition!eigenvalue}
\end{definition}

\begin{definition}[Singular Value Decomposition (SVD)]
	Let $A \in \mathbb{R}^{m \times n}$.
	Then, the singular value decomposition of $A$ is defined to be
	\begin{align*}
		A &= U \Sigma V^*
	\end{align*}
	where the diagonal elements of $\Sigma$ are the singular values of $A$, and the remaining elements of $\Sigma$ are zero.\\
	The columns of $V$ are called the right singular vectors, the columns of $U$ are called the left singular vectors, and the diagonal elements of $\Sigma$ are denoted by $\sigma_i$, where $1 \le i \le \min(m,n)$.
	\index{decomposition!singular value}
\end{definition}

\begin{theorem}
	\begin{align*}
		{\sigma_i}^2 &= \lambda_i \left( A^* A \right)
	\end{align*}
	where $\sigma_i$ are the singular values of $A$, and $\lambda_i$ are the eigenvalues of $A$, for $1 \le i \le \min(m,n)$.
	\index{decomposition!eigenvalue}
	\index{decomposition!singular value}
\end{theorem}

\begin{theorem}
	\begin{align*}
		N(A) &= \vspan\left\{ v_i \Big| \sigma_i = 0 \text{ or } i > \min(m,n) \right\}
	\end{align*}
\end{theorem}

\begin{theorem}
	\begin{align*}
		R(A) &= \vspan\left\{ u_i \Big| \sigma_i \neq 0 \right\}
	\end{align*}
\end{theorem}

\section{Set Transformations}

\begin{definition}[Set transformation]
	For a set of vectors $X$ composed of $x_1,\dots,x_n$,
	\begin{align*}
		x &= X a\\
		&= \sum a_i x_i
	\end{align*}
	If $n$ is finite, then $X$ is a matrix with columns $x_1,\dots,x_n$.
	\index{set transformation}
\end{definition}

\begin{definition}[Linear independence]
	The set $\{x_i\}$ is said to be linearly independent if and only if
	\begin{align*}
		\sum a_i x_i &= 0
	\end{align*}
	implies $a_i = 0$, $\forall i$.\\
	Equivalently, the set $\{x_i\}$ is said to be linearly independent if and only if
	\begin{align*}
		N(X) &= \{0\}
	\end{align*}
	\index{linear independence}
\end{definition}

\begin{theorem}
	A set of orthogonal vectors/functions with positive norm is always linearly independent.
	\index{linear independence}
	\index{orthogonality}
\end{theorem}

\begin{definition}[Completeness]
	As set $\{x_i \in H\}$ is said to be complete if and only if
	\begin{align*}
		\vspan{x_i} &= H
	\end{align*}
	that is, for any $x \in H$, $\exists \varepsilon > 0$ such that
	\begin{align*}
		\left\| x - \sum\limits_{i = 1}^{n} a_i x_i \right\| &< \varepsilon
	\end{align*}
	for a large enough $n$.
	\index{completeness}
\end{definition}

\section{Basis}

\begin{definition}[Schauter basis]
	A set $\{x_i\} \in H$ is said to be a Schauter basis of $H$ if for any $x \in H$, there are unique coefficients $a_i$ such that
	\begin{align*}
		x &= \sum\limits_{i = 1}^{n} a_i x_i
	\end{align*}
	where $n$ is the dimension of $H$, and may be infinite.
	\index{basis!Schauter}
\end{definition}

\begin{definition}[Riesz basis]
	A set $\{x_i\} \in H$ is said to be a Riesz basis of $H$ if it is complete in $H$ and there exist $0 < A \le B < \infty$ such that
	\begin{eqnarray*}
		A \|a\|^2 \le \left\| \sum a_i x_i \right\|^2 \le B \|a\|^2\\
		A \|x\|^2 \le \left\| \sum \langle x x_i \rangle \right\|^2 \le B \|x\|^2
	\end{eqnarray*}
	\index{basis!Riesz}
\end{definition}

\begin{theorem}
	The elements of a Riesz basis are linearly independent.
	\index{basis!Riesz}
\end{theorem}

\section{Projections}

\begin{definition}[Projection]
	The operator $T$ is said to be a projection if
	\begin{align*}
		T T &= T
	\end{align*}
	that is
	\begin{align*}
		T^2 &= T
	\end{align*}
	\index{projection}
\end{definition}

\begin{theorem}
	\begin{align*}
		R(T) \cap N(T) &= \{0\}
	\end{align*}
	where $T$ is a projection.
	\index{projection}
\end{theorem}

\begin{proof}
	Let $y \in R(T)$ and $y \in N(T)$.
	Then,
	\begin{align*}
		T y &= y\\
		T y &= 0
	\end{align*}
	Therefore,
	\begin{align*}
		y &= 0
	\end{align*}
\end{proof}

\begin{theorem}
	\begin{align*}
		H &= R(T) \oplus N(T)
	\end{align*}
	where $T$ is a projection.
	\index{projection}
\end{theorem}

\begin{proof}
	\begin{align*}
		x &= T x + x - T x
	\end{align*}
	Hence, $T x$ is in $R(T)$, and $x - T x$ is in $N(T)$.
	Therefore, as $x$ is in $H$,
	\begin{align*}
		H &= R(T) \cup N(T)
	\end{align*}
	Also,
	\begin{align*}
		R(T) \cap N(T) &= \{0\}
	\end{align*}
	Therefore,
	\begin{align*}
		H &= R(T) \oplus N(T)
	\end{align*}
\end{proof}

\begin{theorem}
	Any projection $T$ can be characterized by the range and the null space of $T$.
	Hence, it can be denoted as
	\begin{align*}
		T &= E_{V W}
	\end{align*}
	where
	\begin{align*}
		V &= R(T)\\
		W &= N(T)
	\end{align*}
	\index{projection}
	\index{range}
	\index{null space}
\end{theorem}

\subsection{Orthogonal Projections}

\begin{definition}[Orthogonal projection]
	A projection $T$ is said to be an orthogonal projection if
	\begin{align*}
		T &= T^*
	\end{align*}
	Hence, as
	\begin{align*}
		N(T) &= N\left( T^* \right)\\
		&= R(T)
	\end{align*}
	or equivalently
	\begin{align*}
		V &= W^{\perp}
	\end{align*}
	it is enough to specify only $V$ to describe a orthogonal projection.
	Hence,
	\begin{align*}
		T &= P_V
	\end{align*}
	\index{projection!orthogonal}
\end{definition}

\begin{theorem}
	For an orthogonal projection $P_V$,
	\begin{align*}
		\langle x_V , x_{V^{\perp}} \rangle &= 0
	\end{align*}
	where
	\begin{align*}
		x_V &= P_V x
	\end{align*}
	\index{projection!orthogonal}
\end{theorem}

\begin{theorem}
	For an orthogonal projection $P_V$,
	\begin{align*}
		\|x\|^2 &= \|x_V\|^2 + \|x_{V^{\perp}}\|^2\\
		\therefore \|x_V\|^2 &\le \|x\|^2
	\end{align*}
	where
	\begin{align*}
		x_V &= P_V x
	\end{align*}
	\index{projection!orthogonal}
\end{theorem}

\begin{theorem}
	\begin{align*}
		x_V &= P_V x\\
		&= \argmin\limits_{v \in V} \|v - x\|^2
	\end{align*}
	\index{projection!orthogonal}
\end{theorem}

\begin{theorem}
	\begin{align*}
		P_V &= V \tilde{V}^*\\
		&= V \left( V^* V \right)^{-1} V^*
	\end{align*}
	\index{projection!orthogonal}
\end{theorem}

\subsection{Oblique Projections}

\begin{definition}[Oblique projection]
	A projection $T$ is said to be an oblique projection if
	\begin{align*}
		T &\neq T^*
	\end{align*}
	Such a projection can be denoted as
	\begin{align*}
		T &= E_{V W}\\
		x &= x_V + x_W
	\end{align*}
	similar to orthogonal projections, but
	\begin{align*}
		\langle x_V , x_W \rangle &\neq 0
	\end{align*}
	\index{projection!oblique}
\end{definition}

\section{Pseudoinverse}

\begin{definition}[Pseudoinverse]
	Let $T$ be a transformation with closed range.
	Then, the pseudoinverse of $T$ is defined to be $T^{\dagger}$, such that
	\begin{align*}
		T^{\dagger} T x &= x\\
		T^{\dagger} y &=
	\end{align*}
	for all $x \in {N(T)}^{\perp}$, and for all $y \in {R(T)}^{\perp}$.
	\index{pseudoinverse}
\end{definition}

\begin{theorem}
	\begin{align*}
		T T^{\dagger} &= P_{R(T)}
	\end{align*}
	\index{pseudoinverse}
\end{theorem}

\begin{theorem}
	\begin{align*}
		T^{\dagger} T &= P_{{N(T)}^{\perp}}
	\end{align*}
	\index{pseudoinverse}
\end{theorem}

\begin{theorem}
	\begin{align*}
		R\left( T^{\dagger} \right) &= N(T)^{\perp}
	\end{align*}
	\index{pseudoinverse}
\end{theorem}

\begin{theorem}
	\begin{align*}
		N\left( T^{\dagger} \right) &= R(T)^{\perp}
	\end{align*}
	\index{pseudoinverse}
\end{theorem}

\begin{theorem}
	If $T^* T$ is invertible,
	\begin{align*}
		T^{\dagger} &= \left( T^* T \right)^{-1} T^*
	\end{align*}
	\index{pseudoinverse}
\end{theorem}

\section{Frames}

\begin{definition}[Frame]
	The set $\{x_i\}$ is said to be a frame of $H$ if and only if there exist $0 < A \le B < \infty$ such that
	\begin{eqnarray*}
		A \|x\|^2 \le \sum \left| \langle x_i,x \rangle \right|^2 \le B \|x\|^2
	\end{eqnarray*}
	for all $x \in H$.
	\index{frame}
\end{definition}

\begin{theorem}
	A frame is a complete set.
	\index{frame}
\end{theorem}

\begin{definition}[Tight frame]
	A frame for which $A = B$ is called a tight frame.
	\index{frame!tight}
\end{definition}

\begin{definition}[Dual frame]

\begin{definition}[Dual frame]
	For any frame $\{x_i\}$ of $H$, and for $x \in H$, a dual frame $\left\{ \tilde{x_i} \right\}$ of $\{x_i\}$ is defined such that
	\begin{align*}
		x &= \sum \langle \tilde{x_i},x \rangle x_i
	\end{align*}
	\index{frame!dual}
\end{definition}

\begin{theorem}
	A dual frame is not unique.
	\index{frame!dual}
\end{theorem}

\begin{definition}[Canonical dual frame]
	The canonical dual frame $\tilde{X}$ of $X$ is defined to be
	\begin{align*}
		\tilde{X} &= \left( X X^* \right)^* X\\
		&= \left( X^{\dagger} \right)^*
	\end{align*}
	If $X$ is a tight frame, with $A = B$,
	\begin{align*}
		\tilde{X} &= \frac{1}{A} X
	\end{align*}
	In finite dimensions, the frame $X$ has more columns than rows, and has full rank.
	\index{frame!canonical dual}
\end{definition}

\begin{theorem}
	A matrices, $X$ is a tight frame with $A = 1$ if its rows are orthonormal, i.e.
	\begin{align*}
		X X^* &= I
	\end{align*}
	\index{frame!tight}
\end{theorem}

\clearpage
\part{Sampling Theory}

\section{Shannon's Interpolation Formula}

\begin{theorem}[Shannon's Interpolation Formula]
	A bandlimited signal limited by $\pm\frac{1}{2 T}$ can be reconstructed from the samples $x(n T)$ from the formula
	\begin{align*}
		x(t) &= \sum\limits_{n = -\infty}^{\infty} x(n T) \sinc\left( \frac{t - n T}{T} \right)
	\end{align*}
	\label{thm:Shannons_Interpolation_Formula}
	\index{Shannon's Interpolation Formula}
\end{theorem}

\clearpage
\printindex

\end{document}
